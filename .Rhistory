##########################################################################
##########################################################################
# Some of the stemming from the SnowballC packages wordsStem(language="spanish") was incorrect or misleading.
#  For example "comunica", "comunicación", "comunicaciones" and "comunidad" were all being grouped with the stem "comunic"
#   For the puurposes of our paper such groupings led to a misleading count for certian concepts and key words.
#   So we manually analyzed all words appearing mroe than 20 times in the full combined corpus and corrected and stems
#     that led to inaccurate groupings of words
# In creating custom stems I need to be careful not to choose a custom stem that already exists.
#   So before applying the custom stems I will combine all the words, keywords adn their stems into one table and
#     check to ensure that none of the new custom stems are already in use
# Select only the word and stem variables from each dataframe
preChavezPdfWordsAndStemsOnly <- select(preChavezPdfTokens, word, stem)
postChavezPdfWordsAndStemsOnly <- select(postChavezPdfTokens, word, stem)
stemKeywordWordsAndStemsOnly <- select(keywordMasterList, word, stem)
# Combine the words and stem from all three dataframes
AllTokensAndKeywords <- bind_rows(list(preChavezPdfWordsAndStemsOnly, postChavezPdfWordsAndStemsOnly, stemKeywordWordsAndStemsOnly) )
# # Now Check to see if any of the propsed stems already exist.
# duplicateStems <- filter(AllTokensAndKeywords, stem=="comunic"|
#                              stem=="comunic"|
#                              stem=="convivn"|
#                              stem=="gratis"|
#                              stem=="hombro"|
#                              stem=="importaci"|
#                              stem=="injust"|
#                              stem=="latinoame"|
#                              stem=="libertado"|
#                              stem=="merce"|
#                              stem=="poblacion"|
#                              stem=="saluda"
#                               )
# Code to check a particular word, stem
#
#  filter(AllTokensAndKeywords, word=="injusticias"| word=="injusto")
# duplicateStems <- group_by(duplicateStems, word)
#
# duplicateStemsUniqueWords <- slice(duplicateStems, 1)
#  write_csv(duplicateStemsUniqueWords, path = "duplicateStemsForAntonioToCheck.csv")
## Now that we are sure the new stems give the proper groupings generate the new stem list
newStems <- constructCustomStemList()
# Some Diagnostics
# filter(preChavezPdfTokens, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")
# filter(postChavezPdfTokens, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")
# filter(keywordMasterList, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")
# filter(newStems, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")
# Update the stems with the custom stems
preChavezPdfTokens <- updateStems(preChavezPdfTokens, newStems)
postChavezPdfTokens <- updateStems(postChavezPdfTokens, newStems)
keywordMasterList <- updateStems(keywordMasterList, newStems)
# ###### DIAGNOSTIC
# #print( filter(preChavezPdfTokens, word=="indígena" | word=="indígenas"), n=Inf )
# checkTerms(tokenTableToCheck = preChavezPdfTokens, typeToCheck = "word", termsToCheck = c("indígena", "indígenas"), printAllOutput = TRUE)
#
# #print( filter(postChavezPdfTokens, word=="indígena" | word=="indígenas"), n=Inf )
# checkTerms(tokenTableToCheck = postChavezPdfTokens, typeToCheck = "word", termsToCheck = c("indígena", "indígenas"), printAllOutput = TRUE)
#
# #rint( filter(keywordMasterList, word=="indígena" | word=="indígenas"), n=Inf )
# checkTerms(tokenTableToCheck = keywordMasterList, typeToCheck = "word", termsToCheck = c("indígena", "indígenas"), printAllOutput = TRUE)
checkAllAreSame(tokenTablesToCheck = list(preChavezPdfTokens, postChavezPdfTokens, keywordMasterList),typeToFilter = "word",termsToCheck = c("indígena", "indígenas"), typeToCheck = "stem")
checkAllAreSame(tokenTablesToCheck = list(preChavezPdfTokens, postChavezPdfTokens, keywordMasterList),typeToFilter = "word",termsToCheck = c("lucha", "lucharon", "luchar", "lucharán"), typeToCheck = "stem")
checkAllAreSame(tokenTablesToCheck = list(preChavezPdfTokens, postChavezPdfTokens, keywordMasterList),typeToFilter = "word",termsToCheck = c("bolívar", "bolivariano", "bolivariana"), typeToCheck = "stem")
checkAllAreSame(tokenTablesToCheck = list(preChavezPdfTokens, postChavezPdfTokens, keywordMasterList),typeToFilter = "word",termsToCheck = c( "patria", "patriota", "patriotismo"), typeToCheck = "stem")
View(newStems)
# First create each category of keywords
customStemList <- tibble::tibble(word= c("bolivariano",
"bolivariana",
"comunica",
"comunicación",
"comunicaciones",
"convivencia",
"crecimiento",
"ciudades",
"esclavizado",
"esclavizada",
"esclavizados",
"gratuita",
"hombros",
"importación",
"importaciones",
"importamos",
"injusticia",
"injusticias",
"latinoamérical",
"latinoamericana",
"latinoamericanas",
"latinoamericano",
"latinoamericanos",
"libertador",
"merced",
"patriota",
"patriotismo",
"poblacional",
"republicana",
"republicano",
"saluda",
"saludado",
"saludan",
"saludo",
"saludó",
"saludos"),
stem=c("boliv",
"boliv",
"comunic",
"comunic",
"comunic",
"conviven",
"crec",
"ciud",
"esclav",
"esclav",
"esclav",
"gratis",
"hombro",
"importación",
"importación",
"importación",
"injust",
"injust",
"latinoamer",
"latinoamer",
"latinoamer",
"latinoamer",
"latinoamer",
"libertador",
"merce",
"patri",
"patri",
"poblacion",
"republ",
"republ",
"saluda",
"saluda",
"saluda",
"saluda",
"saluda",
"saluda")
)
#Dignostic
# print( customStemList, n=Inf )
customStemList
View(newStems)
# First create each category of keywords
customStemList <- tibble::tibble(word= c("bolivariano",
"bolivariana",
"comunica",
"comunicación",
"comunicaciones",
"convivencia",
"crecimiento",
"ciudades",
"esclavizado",
"esclavizada",
"esclavizados",
"gratuita",
"hombros",
"importación",
"importaciones",
"importamos",
"injusticia",
"injusticias",
"latinoamérical",
"latinoamericana",
"latinoamericanas",
"latinoamericano",
"latinoamericanos",
"libertador",
"merced",
"patriota",
"patriotismo",
"poblacional",
"republicana",
"republicano",
"saluda",
"saludado",
"saludan",
"saludo",
"saludó",
"saludos"),
stem=c("boliv",
"boliv",
"comunic",
"comunic",
"comunic",
"conviven",
"crec",
"ciud",
"esclav",
"esclav",
"esclav",
"gratis",
"hombro",
"importación",
"importación",
"importación",
"injust",
"injust",
"latinoamer",
"latinoamer",
"latinoamer",
"latinoamer",
"latinoamer",
"libertador",
"merce",
"patri",
"patri",
"poblacion",
"republ",
"republ",
"saluda",
"saluda",
"saluda",
"saluda",
"saluda",
"saluda")
)
#Dignostic
# print( customStemList, n=Inf )
customStemList
View(customStemList)
library(textbooksPrePostChavez)
################################################################
# cleaning2_StemLemmaFreqPropStore.R
#
# - Read in Tokenized Text generated by cleaning1_ReadTokenizeStore.R
# - Apply Stopwords
# - Stem
# - Lemmatize
# - Calculate Frequencies
# - Calulate Proportions
# - Store
################################################################
rm(list = ls(all = TRUE)) # Clear the workspace
# options(scipen=999) # Turns off scientific notation
#################
# Packages
#################
################
# install.packages("vtable")
# install.packages("textbooksPrePostChavez")
library(textbooksPrePostChavez)
#####################################
### Read in the Tokenized Data   ###
### Tokenized Data was created   ###
### with the "Cleaning" Script   ###
###################################
tokensColumnTypes <- cols(
document = col_character(),
pageGroup = col_double(),
line = col_double(),
word = col_character(),
setName = col_character()
)
preChavezPdfTokens <- read_csv(file = "tokenizedText/tokenizedPreChavez.csv", col_types = tokensColumnTypes)
postChavezPdfTokens <- read_csv(file = "tokenizedText/tokenizedPostChavez.csv", col_types = tokensColumnTypes)
keywordsColumnTypes <- cols(
category = col_character(),
word = col_character()
)
keywordMasterList <-  read_csv(file = "keywords/keywords.csv", col_types = keywordsColumnTypes)
#############################
# Re-Order Variables ###
#############################
preChavezPdfTokens <- select(preChavezPdfTokens, setName, document, pageGroup, line, word)
postChavezPdfTokens <- select(postChavezPdfTokens, setName, document, pageGroup, line, word)
#####################################
###     Remove Stop Words       ###
###################################
# We need to remove uninformative words.
# # In text analysis these are often called "stop words"
# # First I extract a spanish "stop word" set from the {quanteda} package.
spanish_stop_words <- tibble(word = stopwords("spanish"), lexicon = "fromQuanteda")
#Now remove all of the stop words from each of the token sets
preChavezPdfTokens <- anti_join(preChavezPdfTokens, spanish_stop_words)
postChavezPdfTokens <- anti_join(postChavezPdfTokens, spanish_stop_words)
# #Remove all lines of underscores
preChavezPdfTokens <- preChavezPdfTokens[-grep("_+", preChavezPdfTokens$word),]
postChavezPdfTokens <- postChavezPdfTokens[-grep("_+", postChavezPdfTokens$word),]
# Remove Whitespace
preChavezPdfTokens$word <- gsub("\\s+","",preChavezPdfTokens$word)
postChavezPdfTokens$word <- gsub("\\s+","",postChavezPdfTokens$word)
# Remove numeric digits
preChavezPdfTokens <- preChavezPdfTokens[-grep("\\b\\d+\\b", preChavezPdfTokens$word),]
postChavezPdfTokens <- postChavezPdfTokens[-grep("\\b\\d+\\b", postChavezPdfTokens$word),]
# #Remove all words that have any digits in them
preChavezPdfTokens <- filter(preChavezPdfTokens, !str_detect(string = preChavezPdfTokens$word, pattern = "\\d") )
postChavezPdfTokens <- filter(postChavezPdfTokens, !str_detect(string = postChavezPdfTokens$word, pattern = "\\d") )
# #Remove all words that have a dot in them
preChavezPdfTokens <- filter(preChavezPdfTokens, !str_detect(string = preChavezPdfTokens$word, pattern = "\\.") )
postChavezPdfTokens <- filter(postChavezPdfTokens, !str_detect(string = postChavezPdfTokens$word, pattern = "\\.") )
#Remove all words less than the number of letters in the shortest keyword file
shortestKeyword <- min( str_count(keywordMasterList$word) )
preChavezPdfTokens <- filter(preChavezPdfTokens, str_count(preChavezPdfTokens$word)>shortestKeyword )
postChavezPdfTokens <- filter(postChavezPdfTokens, str_count(postChavezPdfTokens$word)>shortestKeyword )
# Correct the encoding from apostrophe symbol ’ to apostrophe symbol ' in the Post Chavez tokens
postChavezPdfTokens$word <- str_replace_all(string = postChavezPdfTokens$word, pattern = "’", replacement = "'")
##########################################################################
##########################################################################
# Steming the words and Keywords ###
# See: https://abndistro.com/post/2019/02/10/tidy-text-mining-in-r/
##########################################################################
##########################################################################
preChavezPdfTokens <- mutate(preChavezPdfTokens, stem = SnowballC::wordStem(word, language = "spanish"))
postChavezPdfTokens <- mutate(postChavezPdfTokens, stem = SnowballC::wordStem(word, language = "spanish"))
keywordMasterList <- mutate(keywordMasterList, stem = SnowballC::wordStem(word, language = "spanish"))
##########################################################################
##########################################################################
# Manual Stemming of Key Words and Keywords in the three datasets
##########################################################################
##########################################################################
# Some of the stemming from the SnowballC packages wordsStem(language="spanish") was incorrect or misleading.
#  For example "comunica", "comunicación", "comunicaciones" and "comunidad" were all being grouped with the stem "comunic"
#   For the puurposes of our paper such groupings led to a misleading count for certian concepts and key words.
#   So we manually analyzed all words appearing mroe than 20 times in the full combined corpus and corrected and stems
#     that led to inaccurate groupings of words
# In creating custom stems I need to be careful not to choose a custom stem that already exists.
#   So before applying the custom stems I will combine all the words, keywords adn their stems into one table and
#     check to ensure that none of the new custom stems are already in use
# Select only the word and stem variables from each dataframe
preChavezPdfWordsAndStemsOnly <- select(preChavezPdfTokens, word, stem)
postChavezPdfWordsAndStemsOnly <- select(postChavezPdfTokens, word, stem)
stemKeywordWordsAndStemsOnly <- select(keywordMasterList, word, stem)
# Combine the words and stem from all three dataframes
AllTokensAndKeywords <- bind_rows(list(preChavezPdfWordsAndStemsOnly, postChavezPdfWordsAndStemsOnly, stemKeywordWordsAndStemsOnly) )
# # Now Check to see if any of the propsed stems already exist.
# duplicateStems <- filter(AllTokensAndKeywords, stem=="comunic"|
#                              stem=="comunic"|
#                              stem=="convivn"|
#                              stem=="gratis"|
#                              stem=="hombro"|
#                              stem=="importaci"|
#                              stem=="injust"|
#                              stem=="latinoame"|
#                              stem=="libertado"|
#                              stem=="merce"|
#                              stem=="poblacion"|
#                              stem=="saluda"
#                               )
# Code to check a particular word, stem
#
#  filter(AllTokensAndKeywords, word=="injusticias"| word=="injusto")
# duplicateStems <- group_by(duplicateStems, word)
#
# duplicateStemsUniqueWords <- slice(duplicateStems, 1)
#  write_csv(duplicateStemsUniqueWords, path = "duplicateStemsForAntonioToCheck.csv")
## Now that we are sure the new stems give the proper groupings generate the new stem list
newStems <- constructCustomStemList()
# Some Diagnostics
# filter(preChavezPdfTokens, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")
# filter(postChavezPdfTokens, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")
# filter(keywordMasterList, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")
# filter(newStems, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")
# Update the stems with the custom stems
preChavezPdfTokens <- updateStems(preChavezPdfTokens, newStems)
postChavezPdfTokens <- updateStems(postChavezPdfTokens, newStems)
keywordMasterList <- updateStems(keywordMasterList, newStems)
# ###### DIAGNOSTIC
# #print( filter(preChavezPdfTokens, word=="indígena" | word=="indígenas"), n=Inf )
# checkTerms(tokenTableToCheck = preChavezPdfTokens, typeToCheck = "word", termsToCheck = c("indígena", "indígenas"), printAllOutput = TRUE)
#
# #print( filter(postChavezPdfTokens, word=="indígena" | word=="indígenas"), n=Inf )
# checkTerms(tokenTableToCheck = postChavezPdfTokens, typeToCheck = "word", termsToCheck = c("indígena", "indígenas"), printAllOutput = TRUE)
#
# #rint( filter(keywordMasterList, word=="indígena" | word=="indígenas"), n=Inf )
# checkTerms(tokenTableToCheck = keywordMasterList, typeToCheck = "word", termsToCheck = c("indígena", "indígenas"), printAllOutput = TRUE)
checkAllAreSame(tokenTablesToCheck = list(preChavezPdfTokens, postChavezPdfTokens, keywordMasterList),typeToFilter = "word",termsToCheck = c("indígena", "indígenas"), typeToCheck = "stem")
checkAllAreSame(tokenTablesToCheck = list(preChavezPdfTokens, postChavezPdfTokens, keywordMasterList),typeToFilter = "word",termsToCheck = c("lucha", "lucharon", "luchar", "lucharán"), typeToCheck = "stem")
checkAllAreSame(tokenTablesToCheck = list(preChavezPdfTokens, postChavezPdfTokens, keywordMasterList),typeToFilter = "word",termsToCheck = c("bolívar", "bolivariano", "bolivariana"), typeToCheck = "stem")
checkAllAreSame(tokenTablesToCheck = list(preChavezPdfTokens, postChavezPdfTokens, keywordMasterList),typeToFilter = "word",termsToCheck = c( "patria", "patriota", "patriotismo"), typeToCheck = "stem")
checkAllAreSame(tokenTablesToCheck = list(preChavezPdfTokens, postChavezPdfTokens, keywordMasterList),typeToFilter = "word",termsToCheck = c( "república", "republicana", "republicano", "repúblicas"), typeToCheck = "stem")
checkAllAreSame(tokenTablesToCheck = list(preChavezPdfTokens, postChavezPdfTokens, keywordMasterList),typeToFilter = "word",termsToCheck = c( "esclavo", "esclava", "esclavizado", "esclavizada", "esclavizados"), typeToCheck = "stem")
checkAllAreSame(tokenTablesToCheck = list(preChavezPdfTokens, postChavezPdfTokens, keywordMasterList),typeToFilter = "word",termsToCheck = c("crecimiento", "crecer"), typeToCheck = "stem")
# identify the folders
current.folder <- "images/"
new.folder <- "/Users/ragan_ra/Dropbox/Indoctrination"
list.files(current.folder, "SDM\\.tif$",full.names=T)
list.files(current.folder)
# identify the folders
current.folder <- "images/"
new.folder <- "/Users/ragan_ra/Dropbox/Indoctrination"
list.files(current.folder)
list.files()
# identify the folders
current.folder <- "images"
# find the files that you want
list.of.files <- list.files(current.folder)
list.of.files
list.files("images")
list.files("/images")
list.files("/images/")
list.files("images")
list.files("images/")
?list.files
list.files(path = "images/")
list.files(path = "images")
source("cleaning2_FreqPropStore.R")
# source("analysis1_Frequency.R")
source("analysis2_Proportions.R")
# identify the folders
current.folder <- "images"
new.folder <- "/Users/ragan_ra/Dropbox/Indoctrination"
# find the files that you want
list.of.files <- list.files(path = "images")
list.of.files
# find the files that you want
list.of.files <- list.files(path = "images", full.names=T)
list.of.files
# find the files that you want
list.of.files <- c("images/top10KeywordLemmasProp.pdf",  "images/top50AllLemmasPropChange50.pdf")
# copy the files to the new folder
file.copy(list.of.files, "/Users/ragan_ra/Dropbox/Indoctrination")
# copy the files to the new folder
file.copy(list.of.files, "/Users/ragan_ra/Dropbox/Indoctrination/graphsForPaper")
# find the files that you want
list.of.files <- c("images/top10KeywordLemmasProp.pdf",  "images/top10KeywordsLemmasPropChange.pdf")
# copy the files to the new folder
file.copy(list.of.files, "/Users/ragan_ra/Dropbox/Indoctrination/graphsForPaper")
# find the files that you want
list.of.files <- c("images/top10KeywordLemmasProp.pdf",  "images/top10KeywordsLemmasPropChange.pdf")
# copy the files to the new folder
file.copy(list.of.files, "/Users/ragan_ra/Dropbox/Indoctrination/graphsForPaper")
################################################################
# regenerateAllGraphs.R
#
# A script to do frequency analysis
################################################################
rm(list = ls(all = TRUE)) # Clear the workspace
# options(scipen=999) # Turns off scientific notation
#################
# Packages
#################
# install.packages("textbooksPrePostChavez")
library(textbooksPrePostChavez)
source("cleaning1_ReadTokenizeStore.R")
source("cleaning2_FreqPropStore.R")
# source("analysis1_Frequency.R")
source("analysis2_Proportions.R")
# source("analysis3_KeywordChanges.R")
# source("analysis4_TFIDF.R")
source("moveSelectedGraphs.R")
# find the files that you want
list.of.files <- list.files("images")
list.of.files
# find the files that you want
list.of.files <- list.files("images", path = TRUE)
# find the files that you want
list.of.files <- list.files("images", path = T)
# find the files that you want
list.of.files <- list.files("images", full.names = TRUE)
list.of.files
# find the files that you want
list.of.files <- list.files("/Users/ragan_ra/Dropbox/Indoctrination/allOutputGraphs", full.names = TRUE)
list.of.files
# find the files that you want to clear
filesToCopy <- list.files("images", full.names = TRUE)
filesToCopy
rm(list = ls(all = TRUE)) # Clear the workspace
library(textbooksPrePostChavez)
# find the files that you want to clear
filesToDelete <- list.files("/Users/ragan_ra/Dropbox/Indoctrination/allOutputGraphs", full.names = TRUE)
# clear the directory
file.remove(filesToDelete)
# find the files that you want to clear
filesToCopy <- list.files("images", full.names = TRUE)
# copy the files to the new folder
file.copy(filesToCopy, "/Users/ragan_ra/Dropbox/Indoctrination/allOutputGraphs")
# find the files that you want to clear
filesToDelete <- list.files("/Users/ragan_ra/Dropbox/Indoctrination/graphsForPaper/", full.names = TRUE)
# clear the directory
file.remove(filesToDelete)
# find the files that you want
list.of.files <- c("images/top10KeywordLemmasProp.pdf",  "images/top10KeywordsLemmasPropChange.pdf", "images/top10AllLemmasPropChange.pdf", "images/top10AllLemmasProp.pdf")
# copy the files to the new folder
file.copy(list.of.files, "/Users/ragan_ra/Dropbox/Indoctrination/graphsForPaper")
################################################################
# regenerateAllGraphs.R
#
# A script to do frequency analysis
################################################################
rm(list = ls(all = TRUE)) # Clear the workspace
# options(scipen=999) # Turns off scientific notation
#################
# Packages
#################
# install.packages("textbooksPrePostChavez")
library(textbooksPrePostChavez)
source("cleaning1_ReadTokenizeStore.R")
source("cleaning2_FreqPropStore.R")
# source("analysis1_Frequency.R")
source("analysis2_Proportions.R")
# source("analysis3_KeywordChanges.R")
# source("analysis4_TFIDF.R")
source("moveSelectedGraphs.R")
source("moveAllGraphs.R")
################################################################
# regenerateAllGraphs.R
#
# A script to do frequency analysis
################################################################
rm(list = ls(all = TRUE)) # Clear the workspace
# options(scipen=999) # Turns off scientific notation
#################
# Packages
#################
# install.packages("textbooksPrePostChavez")
library(textbooksPrePostChavez)
source("cleaning1_ReadTokenizeStore.R")
source("cleaning2_FreqPropStore.R")
source("analysis1_Frequency.R")
source("analysis2_Proportions.R")
source("analysis3_KeywordChanges.R")
source("analysis4_TFIDF.R")
source("moveSelectedGraphs.R")
source("moveAllGraphs.R")
