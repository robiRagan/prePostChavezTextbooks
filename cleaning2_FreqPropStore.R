################################################################
# cleaning2_StemLemmaFreqPropStore.R
#
# - Read in Tokenized Text generated by cleaning1_ReadTokenizeStore.R
# - Apply Stopwords
# - Stem
# - Lemmatize
# - Calculate Frequencies
# - Calulate Proportions
# - Store
################################################################


rm(list = ls(all = TRUE)) # Clear the workspace

# options(scipen=999) # Turns off scientific notation


#################
# Packages
#################
################
# install.packages("vtable")
# install.packages("textbooksPrePostChavez")

library(textbooksPrePostChavez)


#####################################
### Read in the Tokenized Data   ###
### Tokenized Data was created   ###
### with the "Cleaning" Script   ###
###################################

tokensColumnTypes <- cols(
  document = col_character(),
  pageGroup = col_double(),
  line = col_double(),
  word = col_character(),
  setName = col_character()
)

preChavezPdfTokens <- read_csv(file = "tokenizedText/tokenizedPreChavez.csv", col_types = tokensColumnTypes)
postChavezPdfTokens <- read_csv(file = "tokenizedText/tokenizedPostChavez.csv", col_types = tokensColumnTypes)

keywordsColumnTypes <- cols(
  category = col_character(),
  word = col_character()
)

keywordMasterList <-  read_csv(file = "keywords/keywords.csv", col_types = keywordsColumnTypes)




#############################
# Re-Order Variables ###
#############################
preChavezPdfTokens <- select(preChavezPdfTokens, setName, document, pageGroup, line, word)
postChavezPdfTokens <- select(postChavezPdfTokens, setName, document, pageGroup, line, word)


#####################################
###     Remove Stop Words       ###
###################################

# We need to remove uninformative words.
# # In text analysis these are often called "stop words"
# # First I extract a spanish "stop word" set from the {quanteda} package.
spanish_stop_words <- tibble(word = stopwords("spanish"), lexicon = "fromQuanteda")


#Now remove all of the stop words from each of the token sets
preChavezPdfTokens <- anti_join(preChavezPdfTokens, spanish_stop_words)
postChavezPdfTokens <- anti_join(postChavezPdfTokens, spanish_stop_words)

# #Remove all lines of underscores
preChavezPdfTokens <- preChavezPdfTokens[-grep("_+", preChavezPdfTokens$word),]
postChavezPdfTokens <- postChavezPdfTokens[-grep("_+", postChavezPdfTokens$word),]


# Remove Whitespace
preChavezPdfTokens$word <- gsub("\\s+","",preChavezPdfTokens$word)
postChavezPdfTokens$word <- gsub("\\s+","",postChavezPdfTokens$word)


# Remove numeric digits
preChavezPdfTokens <- preChavezPdfTokens[-grep("\\b\\d+\\b", preChavezPdfTokens$word),]
postChavezPdfTokens <- postChavezPdfTokens[-grep("\\b\\d+\\b", postChavezPdfTokens$word),]

# #Remove all words that have any digits in them
preChavezPdfTokens <- filter(preChavezPdfTokens, !str_detect(string = preChavezPdfTokens$word, pattern = "\\d") )
postChavezPdfTokens <- filter(postChavezPdfTokens, !str_detect(string = postChavezPdfTokens$word, pattern = "\\d") )

# #Remove all words that have a dot in them
preChavezPdfTokens <- filter(preChavezPdfTokens, !str_detect(string = preChavezPdfTokens$word, pattern = "\\.") )
postChavezPdfTokens <- filter(postChavezPdfTokens, !str_detect(string = postChavezPdfTokens$word, pattern = "\\.") )

#Remove all words less than the number of letters in the shortest keyword file
shortestKeyword <- min( str_count(keywordMasterList$word) )
preChavezPdfTokens <- filter(preChavezPdfTokens, str_count(preChavezPdfTokens$word)>shortestKeyword )
postChavezPdfTokens <- filter(postChavezPdfTokens, str_count(postChavezPdfTokens$word)>shortestKeyword )

# Correct the encoding from apostrophe symbol ’ to apostrophe symbol ' in the Post Chavez tokens
postChavezPdfTokens$word <- str_replace_all(string = postChavezPdfTokens$word, pattern = "’", replacement = "'")


##########################################################################
##########################################################################
# Steming the words and Keywords ###
# See: https://abndistro.com/post/2019/02/10/tidy-text-mining-in-r/
##########################################################################
##########################################################################


preChavezPdfTokens <- mutate(preChavezPdfTokens, stem = SnowballC::wordStem(word, language = "spanish"))

postChavezPdfTokens <- mutate(postChavezPdfTokens, stem = SnowballC::wordStem(word, language = "spanish"))

keywordMasterList <- mutate(keywordMasterList, stem = SnowballC::wordStem(word, language = "spanish"))


##########################################################################
##########################################################################
# Manual Stemming of Key Words and Keywords in the three datasets
##########################################################################
##########################################################################

# Some of the stemming from the SnowballC packages wordsStem(language="spanish") was incorrect or misleading.
#  For example "comunica", "comunicación", "comunicaciones" and "comunidad" were all being grouped with the stem "comunic"
#   For the puurposes of our paper such groupings led to a misleading count for certian concepts and key words.
#   So we manually analyzed all words appearing mroe than 20 times in the full combined corpus and corrected and stems
#     that led to inaccurate groupings of words

# In creating custom stems I need to be careful not to choose a custom stem that already exists.
#   So before applying the custom stems I will combine all the words, keywords adn their stems into one table and
#     check to ensure that none of the new custom stems are already in use

# Select only the word and stem variables from each dataframe
preChavezPdfWordsAndStemsOnly <- select(preChavezPdfTokens, word, stem)
postChavezPdfWordsAndStemsOnly <- select(postChavezPdfTokens, word, stem)
stemKeywordWordsAndStemsOnly <- select(keywordMasterList, word, stem)

# Combine the words and stem from all three dataframes
AllTokensAndKeywords <- bind_rows(list(preChavezPdfWordsAndStemsOnly, postChavezPdfWordsAndStemsOnly, stemKeywordWordsAndStemsOnly) )

# # Now Check to see if any of the propsed stems already exist.
# duplicateStems <- filter(AllTokensAndKeywords, stem=="comunic"|
#                              stem=="comunic"|
#                              stem=="convivn"|
#                              stem=="gratis"|
#                              stem=="hombro"|
#                              stem=="importaci"|
#                              stem=="injust"|
#                              stem=="latinoame"|
#                              stem=="libertado"|
#                              stem=="merce"|
#                              stem=="poblacion"|
#                              stem=="saluda"
#                               )

# Code to check a particular word, stem
#
#  filter(AllTokensAndKeywords, word=="injusticias"| word=="injusto")

# duplicateStems <- group_by(duplicateStems, word)
#
# duplicateStemsUniqueWords <- slice(duplicateStems, 1)

#  write_csv(duplicateStemsUniqueWords, path = "duplicateStemsForAntonioToCheck.csv")


## Now that we are sure the new stems give the proper groupings generate the new stem list
newStems <- constructCustomStemList()



# Some Diagnostics


# filter(preChavezPdfTokens, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")
# filter(postChavezPdfTokens, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")
# filter(keywordMasterList, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")
# filter(newStems, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")





# Update the stems with the custom stems

preChavezPdfTokens <- updateStems(preChavezPdfTokens, newStems)

postChavezPdfTokens <- updateStems(postChavezPdfTokens, newStems)

keywordMasterList <- updateStems(keywordMasterList, newStems)


# ###### DIAGNOSTIC
# #print( filter(preChavezPdfTokens, word=="indígena" | word=="indígenas"), n=Inf )
# checkTerms(tokenTableToCheck = preChavezPdfTokens, typeToCheck = "word", termsToCheck = c("indígena", "indígenas"), printAllOutput = TRUE)
#
# #print( filter(postChavezPdfTokens, word=="indígena" | word=="indígenas"), n=Inf )
# checkTerms(tokenTableToCheck = postChavezPdfTokens, typeToCheck = "word", termsToCheck = c("indígena", "indígenas"), printAllOutput = TRUE)
#
# #rint( filter(keywordMasterList, word=="indígena" | word=="indígenas"), n=Inf )
# checkTerms(tokenTableToCheck = keywordMasterList, typeToCheck = "word", termsToCheck = c("indígena", "indígenas"), printAllOutput = TRUE)

#
# checkAllAreSame(tokenTablesToCheck = list(preChavezPdfTokens, postChavezPdfTokens, keywordMasterList),typeToFilter = "word",termsToCheck = c("indígena", "indígenas"), typeToCheck = "stem")
#
#
# checkAllAreSame(tokenTablesToCheck = list(preChavezPdfTokens, postChavezPdfTokens, keywordMasterList),typeToFilter = "word",termsToCheck = c("lucha", "lucharon", "luchar", "lucharán"), typeToCheck = "stem")
#
#
# checkAllAreSame(tokenTablesToCheck = list(preChavezPdfTokens, postChavezPdfTokens, keywordMasterList),typeToFilter = "word",termsToCheck = c("bolívar", "bolivariano", "bolivariana"), typeToCheck = "stem")
#
#
# checkAllAreSame(tokenTablesToCheck = list(preChavezPdfTokens, postChavezPdfTokens, keywordMasterList),typeToFilter = "word",termsToCheck = c( "patria", "patriota", "patriotismo"), typeToCheck = "stem")
#
#
# checkAllAreSame(tokenTablesToCheck = list(preChavezPdfTokens, postChavezPdfTokens, keywordMasterList),typeToFilter = "word",termsToCheck = c( "república", "republicana", "republicano", "repúblicas"), typeToCheck = "stem")
#
# checkAllAreSame(tokenTablesToCheck = list(preChavezPdfTokens, postChavezPdfTokens, keywordMasterList),typeToFilter = "word",termsToCheck = c( "esclavo", "esclava", "esclavizado", "esclavizada", "esclavizados"), typeToCheck = "stem")
#
# checkAllAreSame(tokenTablesToCheck = list(preChavezPdfTokens, postChavezPdfTokens, keywordMasterList),typeToFilter = "word",termsToCheck = c("crecimiento", "crecer"), typeToCheck = "stem")


# Select only the word and stem variables from each dataframe
preWordsAndStemsOnly <- select(preChavezPdfTokens, word, stem)
postWordsAndStemsOnly <- select(postChavezPdfTokens, word, stem)
keywordWordsAndStemsOnly <- select(keywordMasterList, word, stem)

# Combine the words and stem from all three dataframes
allWordsAndStems <- bind_rows(list(preWordsAndStemsOnly, postWordsAndStemsOnly) )



# ###### DIAGNOSTIC
# print( filter(preChavezPdfTokens, word=="injusticia" | word=="injusticias"), n=Inf )
# print( filter(postChavezPdfTokens, word=="injusticia" | word=="injusticias"), n=Inf )
#
# print( filter(keywordMasterList, word=="injusticia" | word=="injusticias"), n=Inf )







##########################################################################
##########################################################################
# Lemmatization of the words and Keywords ###
# See: http://www.bernhardlearns.com/2017/04/cleaning-words-with-r-stemming.html
##########################################################################
##########################################################################

# To create lemmas I use a simple approach. For a given Stem I find the most common base word across both pre-chavez, post-chavez and the keywords



# Group the dataframe by the stems
AllTokensAndKeywordsByStem <- group_by(allWordsAndStems, stem)
AllTokensAndKeywordsByWord <- group_by(allWordsAndStems, word)

# For each stem select the word that is the most common
AllTokensAndKeywordsLemmas <- summarize(AllTokensAndKeywordsByStem, lemma = names(which.max(table(word))))

# For diagnostic purposes create two count tables.
#   - One listing the frequency of the word
#   - One listing the frequency of stem

freqOfStemWord <- count(AllTokensAndKeywordsByStem, stem, sort = TRUE)
freqOfWords <- count(AllTokensAndKeywordsByWord, word, sort = TRUE)


# # Ungroup the dataframe by the stems
# AllTokensAndKeywordsByStem <- ungroup(AllTokensAndKeywordsByStem)
# AllTokensAndKeywordsByWord <- ungroup(AllTokensAndKeywordsByWord)


# # Now create a lemma variable in each of the original 3 data frames based on stem
preChavezPdfTokens <- left_join(x = preChavezPdfTokens, y = AllTokensAndKeywordsLemmas, by="stem")
postChavezPdfTokens <- left_join(x = postChavezPdfTokens, y = AllTokensAndKeywordsLemmas, by="stem")
keywordMasterList <- left_join(x = keywordMasterList, y = AllTokensAndKeywordsLemmas, by="stem")




# filter(preChavezPdfTokens, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")
# filter(postChavezPdfTokens, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")
# filter(keywordMasterList, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")
# filter(newStems, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")



#
#####################################
## ALL WORD and ALL STEM FREQUENCY ##
####################################

# Construct a word count
preChavezWordFrequency <- count(x = preChavezPdfTokens, word, sort = TRUE, name = "wordCount")
postChavezWordFrequency <- count(x = postChavezPdfTokens, word, sort = TRUE, name = "wordCount")

# Add the counts to the Tokens Data Frames
preChavezPdfTokens <- left_join(x = preChavezPdfTokens, y = preChavezWordFrequency, by="word")
postChavezPdfTokens <- left_join(x = postChavezPdfTokens, y = postChavezWordFrequency, by="word")

# Check the counts as a diagnostic
# arrange(preChavezPdfTokens, desc(wordCount))
# arrange(postChavezPdfTokens, desc(wordCount))
# arrange(preChavezPdfTokens, wordCount)
# arrange(postChavezPdfTokens, wordCount)



# Construct a stemWord count
preChavezStemWordFrequency <- count(x = preChavezPdfTokens, stem, sort = TRUE, name = "stemCount")
postChavezStemWordFrequency <- count(x = postChavezPdfTokens, stem, sort = TRUE, name = "stemCount")

# Add the counts to the Tokens Data Frames
preChavezPdfTokens <- left_join(x = preChavezPdfTokens, y = preChavezStemWordFrequency, by="stem")
postChavezPdfTokens <- left_join(x = postChavezPdfTokens, y = postChavezStemWordFrequency, by="stem")

# Check the counts as a diagnostic
# arrange(preChavezPdfTokens, desc(stemCount))
# arrange(postChavezPdfTokens, desc(stemCount))
# arrange(preChavezPdfTokens, stemCount)
# arrange(postChavezPdfTokens, stemCount)



# Construct a lemma count
preChavezLemmaFrequency <- count(x = preChavezPdfTokens, lemma, sort = TRUE, name = "lemmaCount")
postChavezLemmaFrequency <- count(x = postChavezPdfTokens, lemma, sort = TRUE, name = "lemmaCount")

# Add the counts to the Tokens Data Frames
preChavezPdfTokens <- left_join(x = preChavezPdfTokens, y = preChavezLemmaFrequency, by="lemma")
postChavezPdfTokens <- left_join(x = postChavezPdfTokens, y = postChavezLemmaFrequency, by="lemma")

# # Check the counts as a diagnostic
# arrange(preChavezPdfTokens, desc(lemmaCount))
# arrange(postChavezPdfTokens, desc(lemmaCount))
# arrange(preChavezPdfTokens,  lemmaCount)
# arrange(postChavezPdfTokens, lemmaCount)





# Make sure that the lemma count and stem count are the same if using no custom lemmas
sum(preChavezPdfTokens$stemCount - preChavezPdfTokens$lemmaCount) # Should == 0
sum(postChavezPdfTokens$stemCount - postChavezPdfTokens$lemmaCount) # Should == 0


# Combine the sets to look at all words, stems and lemmas at the same time.

allTokensPdfTokensOldCounts <- bind_rows(preChavezPdfTokens, postChavezPdfTokens)

allTokensPdfTokens <- select(allTokensPdfTokensOldCounts, setName, document, pageGroup, line, word, stem, lemma)


# Update the counts for the combined Token List

# Construct a word count
allTokensPdfTokensWordFrequency <- count(x = allTokensPdfTokens, word, sort = TRUE, name = "wordCount")

# Add the counts to the Tokens Data Frames
allTokensPdfTokens <- left_join(x = allTokensPdfTokens, y = allTokensPdfTokensWordFrequency, by="word")

# Construct a stem count
allTokensPdfTokensStemFrequency <- count(x = allTokensPdfTokens, stem, sort = TRUE, name = "stemCount")

# Add the counts to the Tokens Data Frames
allTokensPdfTokens <- left_join(x = allTokensPdfTokens, y = allTokensPdfTokensStemFrequency, by="stem")


# Construct a lemma count
allTokensPdfTokensLemmaFrequency <- count(x = allTokensPdfTokens, lemma, sort = TRUE, name = "lemmaCount")

# Add the counts to the Tokens Data Frames
allTokensPdfTokens <- left_join(x = allTokensPdfTokens, y = allTokensPdfTokensLemmaFrequency, by="lemma")



# For diagnostic purposes add the difference between word counts and stem/lemma counts
preChavezPdfTokensTemp <- mutate(preChavezPdfTokens, wordMinusStemCount = wordCount - stemCount)
postChavezPdfTokensTemp <- mutate(postChavezPdfTokens, wordMinusStemCount = wordCount - stemCount)
allTokensPdfTokensTemp <- mutate(allTokensPdfTokens, wordMinusStemCount = wordCount - stemCount)

# # Check the differences as a diagnostic
# arrange(preChavezPdfTokensTemp, desc(wordMinusStemCount) )
# arrange(postChavezPdfTokensTemp, desc(wordMinusStemCount) )
# arrange(allTokensPdfTokensTemp, desc(wordMinusStemCount) )
#
#
# arrange(preChavezPdfTokensTemp, wordMinusStemCount)
# arrange(postChavezPdfTokensTemp, wordMinusStemCount)
# arrange(allTokensPdfTokensTemp, wordMinusStemCount)



## Common Words Pre
preUniqueWords <- group_by(preChavezPdfTokensTemp, word)

preUniqueWords <- slice(preUniqueWords, 1)

preUniqueWords <- ungroup(preUniqueWords)

preUniqueWords <- select(preUniqueWords, word, stem, lemma, wordCount, stemCount, lemmaCount, wordMinusStemCount)

preUniqueWords <- arrange(preUniqueWords, desc(wordCount) )


## Common Words Post
postUniqueWords <- group_by(postChavezPdfTokensTemp, word)

postUniqueWords <- slice(postUniqueWords, 1)

postUniqueWords <- ungroup(postUniqueWords)

postUniqueWords <- select(postUniqueWords, word, stem, lemma, wordCount, stemCount, lemmaCount, wordMinusStemCount)

postUniqueWords <- arrange(postUniqueWords, desc(wordCount) )


## Common Lemmas Pre
preUniqueLemmas <- group_by(preChavezPdfTokensTemp, lemma)

preUniqueLemmas <- slice(preUniqueLemmas, 1)

preUniqueLemmas <- ungroup(preUniqueLemmas)

preUniqueLemmas <- select(preUniqueLemmas, word, stem, lemma, wordCount, stemCount, lemmaCount, wordMinusStemCount)

preUniqueLemmas <- arrange(preUniqueLemmas, desc(lemmaCount) )


## Common Lemmas Post
postUniqueLemmas <- group_by(postChavezPdfTokensTemp, lemma)

postUniqueLemmas <- slice(postUniqueLemmas, 1)

postUniqueLemmas <- ungroup(postUniqueLemmas)

postUniqueLemmas <- select(postUniqueLemmas, word, stem, lemma, wordCount, stemCount, lemmaCount, wordMinusStemCount)

postUniqueLemmas <- arrange(postUniqueLemmas, desc(lemmaCount) )


## Common words overall
mergedSetsUniqueWords <- group_by(allTokensPdfTokensTemp, lemma)

mergedSetsUniqueWords <- slice(mergedSetsUniqueWords, 1)

mergedSetsUniqueWords <- ungroup(mergedSetsUniqueWords)

mergedSetsUniqueWords <- select(mergedSetsUniqueWords, word, stem, lemma, wordCount, stemCount, lemmaCount, wordMinusStemCount)

# # Diagnostic Filters
#
# filter(allTokensPdfTokensTemp, lemma=="repartición")
# filter(mergedSetsUniqueWords, lemma=="repartición")
# filter(preChavezPdfTokens, lemma=="repartición")
# filter(postChavezPdfTokens, lemma=="repartición")
#
# filter(allTokensPdfTokensTemp, lemma=="distribución")
# filter(mergedSetsUniqueWords, lemma=="distribución")
# filter(preChavezPdfTokens, lemma=="distribución")
# filter(postChavezPdfTokens, lemma=="distribución")
#
#
#
#
#
#
#
# filter(preUniqueWords, word=="injusticia" | word=="reparticion" | word=="distribucion")
# filter(postUniqueWords, word=="injusticia" | word=="reparticion" | word=="distribucion")
#
# filter(preUniqueLemmas, lemma=="injusticias" | lemma=="reparticion" | lemma=="distribucion")
# filter(postUniqueLemmas, lemma=="injusticias" | lemma=="reparticion" | lemma=="distribucion")
#
#
#
#
#
#
#
#
# oneHundredMostCommonLemmasPre <- arrange(preUniqueLemmas, desc(lemmaCount) )
#
# oneHundredMostCommonLemmasPre <- slice(oneHundredMostCommonLemmasPre, 1:100)
#
# oneHundredMostCommonLemmasPre <- select(oneHundredMostCommonLemmasPre, lemma, lemmaCount)
#
# oneHundredMostCommonLemmasPre <- mutate(oneHundredMostCommonLemmasPre, keywordSyn = "none")
#
# write_csv(oneHundredMostCommonLemmasPre, path = "csvsToReview/oneHundredLemmasPre.csv")
#
#
#
# oneHundredMostCommonLemmasPost <- arrange(postUniqueLemmas, desc(lemmaCount) )
#
# oneHundredMostCommonLemmasPost <- slice(oneHundredMostCommonLemmasPost, 1:100)
#
# oneHundredMostCommonLemmasPost <- select(oneHundredMostCommonLemmasPost, lemma, lemmaCount)
#
# oneHundredMostCommonLemmasPost <- mutate(oneHundredMostCommonLemmasPost, keywordSyn = "none")
#
# write_csv(oneHundredMostCommonLemmasPost, path = "csvsToReview/oneHundredLemmasPost.csv")
#
#
# keywordMasterListForReview <- select(keywordMasterList, category, lemma)
# write_csv(keywordMasterListForReview, path = "csvsToReview/keywordMasterList.csv")








#####################################
## ALL WORD and ALL STEM Proportion ##
####################################



# Construct a word proportion
preChavezWordProportion <- tibble(word=preChavezWordFrequency$word, wordProp = ( preChavezWordFrequency$wordCount/sum(preChavezWordFrequency$wordCount) ) )
postChavezWordProportion <- tibble(word=postChavezWordFrequency$word, wordProp = ( postChavezWordFrequency$wordCount/sum(postChavezWordFrequency$wordCount) ) )

# Add the proportions to the Tokens Data Frames
preChavezPdfTokens <- left_join(x = preChavezPdfTokens, y = preChavezWordProportion, by="word")
postChavezPdfTokens <- left_join(x = postChavezPdfTokens, y = postChavezWordProportion, by="word")


# Check the counts as a diagnostic
arrange(preChavezPdfTokens, desc(wordProp))
arrange(postChavezPdfTokens, desc(wordProp))
arrange(preChavezPdfTokens, wordProp)
arrange(postChavezPdfTokens, wordProp)




# Construct a stem proportion
preChavezStemWordProportion <- tibble(stem=preChavezStemWordFrequency$stem, stemProp = ( preChavezStemWordFrequency$stemCount/sum(preChavezStemWordFrequency$stemCount) ) )
postChavezStemWordProportion <- tibble(stem=postChavezStemWordFrequency$stem, stemProp = ( postChavezStemWordFrequency$stemCount/sum(postChavezStemWordFrequency$stemCount) ) )

# Add the proportions to the Tokens Data Frames
preChavezPdfTokens <- left_join(x = preChavezPdfTokens, y = preChavezStemWordProportion, by="stem")
postChavezPdfTokens <- left_join(x = postChavezPdfTokens, y = postChavezStemWordProportion, by="stem")


# Check the counts as a diagnostic
arrange(preChavezPdfTokens, desc(stemProp))
arrange(postChavezPdfTokens, desc(stemProp))
arrange(preChavezPdfTokens, stemProp)
arrange(postChavezPdfTokens, stemProp)



# Construct a lemma proportion
preChavezLemmaProportion <- tibble(lemma=preChavezLemmaFrequency$lemma, lemmaProp = ( preChavezLemmaFrequency$lemmaCount/sum(preChavezLemmaFrequency$lemmaCount) ) )
postChavezLemmaProportion <- tibble(lemma=postChavezLemmaFrequency$lemma, lemmaProp = ( postChavezLemmaFrequency$lemmaCount/sum(postChavezLemmaFrequency$lemmaCount) ) )

# Add the proportions to the Tokens Data Frames
preChavezPdfTokens <- left_join(x = preChavezPdfTokens, y = preChavezLemmaProportion, by="lemma")
postChavezPdfTokens <- left_join(x = postChavezPdfTokens, y = postChavezLemmaProportion, by="lemma")



# # # Code to check a particular word, stem or lemma.

# filter(preChavezPdfTokens, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")
# filter(postChavezPdfTokens, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")
# filter(keywordMasterList, word=="injusta" | word=="injustamente" | word=="injusto" | word=="injusticia" | word=="injustas" | word=="injusticias")

combinedTokens <- bind_rows(preChavezPdfTokens, postChavezPdfTokens)

combinedUniqueTokens <- bind_rows(preUniqueWords, postUniqueWords)



print( filter(preChavezPdfTokens, word=="repartición" | word=="distribución"), n=Inf )
print( filter(postChavezPdfTokens, word=="repartición" | word=="distribución"), n=Inf )
print( filter(combinedUniqueTokens, word=="repartición" | word=="distribución"), n=Inf )

print( filter(combinedUniqueTokens, stem=="boliv"), n=Inf )


print( filter(keywordMasterList, word=="repartición" | word=="distribución"), n=Inf )



print( filter(preUniqueWords, lemma=="repartición" | lemma=="distribución"), n=Inf )
print( filter(postUniqueWords, lemma=="repartición" | lemma=="distribución"), n=Inf )
print( filter(combinedUniqueTokens, lemma=="repartición" | lemma=="distribución"), n=Inf )

print( filter(keywordMasterList, lemma=="repartición" | lemma=="distribución"), n=Inf )






print( filter(preUniqueWords, lemma=="injusticia" | lemma=="injusticias"), n=Inf )
print( filter(postUniqueWords, lemma=="injusticia" | lemma=="injusticias"), n=Inf )
print( filter(combinedUniqueTokens, lemma=="injusticia" | lemma=="injusticias"), n=Inf )

print( filter(keywordMasterList, lemma=="injusticia" | lemma=="injusticias"), n=Inf )


print( filter(preChavezPdfTokens, word=="injusticia" | word=="injusticias"), n=Inf )
print( filter(postChavezPdfTokens, word=="injusticia" | word=="injusticias"), n=Inf )
print( filter(combinedUniqueTokens,  word=="injusticia" | word=="injusticias"), n=Inf )

print( filter(keywordMasterList, word=="injusticia" | word=="injusticias"), n=Inf )


print( arrange(combinedUniqueTokens, desc(wordCount)), n=30)



# Lemma distribución






# filter(preChavezPdfTokens, word=="ciudades" | word=="ciudad")
# filter(postChavezPdfTokens, word=="ciudades" | word=="ciudad")
# filter(keywordMasterList, word=="ciudades" | word=="ciudad")
#
# filter(preChavezPdfTokens, lemma=="ciudades" | lemma=="ciudad")
# filter(postChavezPdfTokens, lemma=="ciudades" | lemma=="ciudad")
# filter(keywordMasterList, lemma=="ciudades" | lemma=="ciudad")



### If any keyword lemmas were lost in teh join replace them with the correct word.

keywordMasterList$lemma <- ifelse(is.na(keywordMasterList$lemma), keywordMasterList$word, keywordMasterList$lemma)


# Write out
#
# keywordMasterList
# preChavezPdfTokens
# postChavezPdfTokens

write_csv(x = keywordMasterList, path = "tokenizedText/keywordMasterListTable.csv")

write_csv(x = preChavezPdfTokens, path = "tokenizedText/preChavezPdfTokenTable.csv")

write_csv(x = postChavezPdfTokens, path = "tokenizedText/postChavezPdfTokenTable.csv")



